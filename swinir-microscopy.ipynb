{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c7fb9d7",
   "metadata": {
    "id": "ZUVGdOVRfjvd",
    "papermill": {
     "duration": 0.009152,
     "end_time": "2025-12-15T17:36:50.862617",
     "exception": false,
     "start_time": "2025-12-15T17:36:50.853465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f9754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:36:50.876911Z",
     "iopub.status.busy": "2025-12-15T17:36:50.876273Z",
     "iopub.status.idle": "2025-12-15T17:38:09.714684Z",
     "shell.execute_reply": "2025-12-15T17:38:09.713807Z"
    },
    "executionInfo": {
     "elapsed": 52687,
     "status": "ok",
     "timestamp": 1761188748562,
     "user": {
      "displayName": "Milson Feliciano",
      "userId": "01049392519473140029"
     },
     "user_tz": -420
    },
    "id": "dFp1TbRiguA9",
    "outputId": "c79514a2-9cc3-451b-e8f3-a0ef6dcb839b",
    "papermill": {
     "duration": 78.846629,
     "end_time": "2025-12-15T17:38:09.716024",
     "exception": false,
     "start_time": "2025-12-15T17:36:50.869395",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# PART 1: Kaggle Environment Initialization\n",
    "# ======================================================================================\n",
    "# --- Install Libraries ---\n",
    "!pip install lpips scikit-image imagecodecs pytorch-msssim --quiet\n",
    "\n",
    "# --- Clone SwinIR Repository into the working directory ---\n",
    "import os\n",
    "if not os.path.exists('/kaggle/working/SwinIR'):\n",
    "    print(\"Cloning SwinIR repository...\")\n",
    "    # We clone into /kaggle/working/ which is a writable directory\n",
    "    !git clone https://github.com/JingyunLiang/SwinIR.git /kaggle/working/SwinIR\n",
    "else:\n",
    "    print(\"SwinIR repository already exists.\")\n",
    "\n",
    "# Change the current directory to the cloned repository\n",
    "os.chdir('/kaggle/working/SwinIR')\n",
    "\n",
    "# --- Download Pre-trained Model ---\n",
    "import requests\n",
    "# This is the correct URL for the model you want to use\n",
    "pretrained_model_url = \"https://github.com/JingyunLiang/SwinIR/releases/download/v0.0/001_classicalSR_DIV2K_s48w8_SwinIR-M_x4.pth\"\n",
    "pretrained_model_path = \"swinir_pretrained_x4.pth\"\n",
    "if not os.path.exists(pretrained_model_path):\n",
    "    print(\"Downloading pre-trained SwinIR model...\")\n",
    "    r = requests.get(pretrained_model_url, allow_redirects=True)\n",
    "    open(pretrained_model_path, 'wb').write(r.content)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"Pre-trained model already downloaded.\")\n",
    "\n",
    "# --- Kaggle Path Configuration ---\n",
    "import torch\n",
    "\n",
    "# --- Dataset ---\n",
    "dataset_name = 'grayscale-microscopy'\n",
    "dataset_root = f'/kaggle/input/{dataset_name}/Split Dataset'\n",
    "\n",
    "# Define paths for the training and validation sets\n",
    "base_training_path = f'{dataset_root}/train'\n",
    "base_validation_path = f'{dataset_root}/val'\n",
    "\n",
    "# --- Global Configuration ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"\\n--- Kaggle Paths ---\")\n",
    "print(f\"Training data path: {base_training_path}\")\n",
    "print(f\"Validation data path: {base_validation_path}\")\n",
    "print(f\"Pre-trained model path: {os.getcwd()}/{pretrained_model_path}\")\n",
    "print(\"--------------------\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "print(\"✅ Kaggle setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a95b8b",
   "metadata": {
    "id": "5k29sy15gKlC",
    "papermill": {
     "duration": 0.023239,
     "end_time": "2025-12-15T17:38:09.763624",
     "exception": false,
     "start_time": "2025-12-15T17:38:09.740385",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Class Define "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34351d9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:38:09.814743Z",
     "iopub.status.busy": "2025-12-15T17:38:09.814331Z",
     "iopub.status.idle": "2025-12-15T17:38:19.948910Z",
     "shell.execute_reply": "2025-12-15T17:38:19.947987Z"
    },
    "executionInfo": {
     "elapsed": 11038,
     "status": "ok",
     "timestamp": 1761188806618,
     "user": {
      "displayName": "Milson Feliciano",
      "userId": "01049392519473140029"
     },
     "user_tz": -420
    },
    "id": "TJsd90NOgQUr",
    "outputId": "b7687339-8b55-44e5-a096-21eeaf7e2177",
    "papermill": {
     "duration": 10.163386,
     "end_time": "2025-12-15T17:38:19.950188",
     "exception": false,
     "start_time": "2025-12-15T17:38:09.786802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# PART 2: CENTRAL DEFINITIONS\n",
    "# ======================================================================================\n",
    "# --- Library Imports ---\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import tifffile\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.ops import DeformConv2d\n",
    "from models.network_swinir import SwinIR # Make sure SwinIR is cloned and accessible\n",
    "\n",
    "# ======================================================================================\n",
    "# 1. DATASET DEFINITION FOR SINGLE-FRAME SUPER-RESOLUTION (SFSR)\n",
    "# ======================================================================================\n",
    "class MicroscopicImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset loader for Single-Frame Super-Resolution (SFSR).\n",
    "    It loads one LR image and its corresponding HR ground truth.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dir, lr_type, hr_patch_size, scale):\n",
    "        self.hr_patch_size = hr_patch_size\n",
    "        self.scale = scale\n",
    "        self.lr_patch_size = hr_patch_size // scale\n",
    "        self.image_pairs = []\n",
    "\n",
    "        # Find all corresponding LR and HR image pairs\n",
    "        for root, dirs, _ in os.walk(base_dir):\n",
    "            if 'ground_truth' in dirs and lr_type in dirs:\n",
    "                hr_dir = os.path.join(root, 'ground_truth')\n",
    "                lr_dir = os.path.join(root, lr_type)\n",
    "                for hr_file in os.listdir(hr_dir):\n",
    "                    if hr_file.endswith((\".tif\", \".tiff\")):\n",
    "                        base_name = os.path.splitext(hr_file)[0]\n",
    "                        lr_filename = f\"{base_name}_01.png\"\n",
    "                        lr_path = os.path.join(lr_dir, lr_filename)\n",
    "                        hr_path = os.path.join(hr_dir, hr_file)\n",
    "                        if os.path.exists(lr_path):\n",
    "                            self.image_pairs.append((lr_path, hr_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_path, hr_path = self.image_pairs[idx]\n",
    "        \n",
    "        # Load images: LR as grayscale, HR from .tif file\n",
    "        lr_img = cv2.imread(lr_path, cv2.IMREAD_GRAYSCALE)\n",
    "        hr_img = tifffile.imread(hr_path)\n",
    "        \n",
    "        # Normalize HR image to 8-bit (0-255) if it's not already\n",
    "        if hr_img.dtype != np.uint8:\n",
    "            hr_img = cv2.normalize(hr_img, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "        \n",
    "        # --- Randomly crop corresponding patches ---\n",
    "        h, w = lr_img.shape\n",
    "        rand_h = random.randint(0, h - self.lr_patch_size)\n",
    "        rand_w = random.randint(0, w - self.lr_patch_size)\n",
    "        \n",
    "        lr_patch = lr_img[rand_h:rand_h + self.lr_patch_size, rand_w:rand_w + self.lr_patch_size]\n",
    "        \n",
    "        hr_h_start, hr_w_start = rand_h * self.scale, rand_w * self.scale\n",
    "        hr_patch = hr_img[hr_h_start:hr_h_start + self.hr_patch_size, hr_w_start:hr_w_start + self.hr_patch_size]\n",
    "        \n",
    "        # --- Data Augmentation ---\n",
    "        if random.random() > 0.5: lr_patch, hr_patch = cv2.flip(lr_patch, 1), cv2.flip(hr_patch, 1) # Horizontal flip\n",
    "        if random.random() > 0.5: lr_patch, hr_patch = cv2.flip(lr_patch, 0), cv2.flip(hr_patch, 0) # Vertical flip\n",
    "        if random.random() > 0.5: lr_patch, hr_patch = cv2.rotate(lr_patch, cv2.ROTATE_90_CLOCKWISE), cv2.rotate(hr_patch, cv2.ROTATE_90_CLOCKWISE)\n",
    "        \n",
    "        # --- Convert to Tensors and Normalize to [0, 1] ---\n",
    "        lr_tensor = torch.from_numpy(lr_patch.copy()).float().unsqueeze(0) / 255.0\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).float().unsqueeze(0) / 255.0\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n",
    "\n",
    "# ======================================================================================\n",
    "# 2. DATASET DEFINITION FOR MULTI-FRAME SUPER-RESOLUTION (MFSR)\n",
    "# ======================================================================================\n",
    "class MFSR_MicroscopicImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset loader for Multi-Frame Super-Resolution (MFSR).\n",
    "    It loads a burst of LR images and their single corresponding HR ground truth.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dir, lr_type, hr_patch_size, scale, num_frames):\n",
    "        self.hr_patch_size, self.scale, self.num_frames = hr_patch_size, scale, num_frames\n",
    "        self.lr_patch_size = hr_patch_size // scale\n",
    "        self.hr_image_paths, self.lr_image_roots = [], {}\n",
    "        \n",
    "        # Find all HR images and map them to their corresponding LR burst folder\n",
    "        for root, dirs, _ in os.walk(base_dir):\n",
    "            if 'ground_truth' in dirs and lr_type in dirs:\n",
    "                hr_dir, lr_dir = os.path.join(root, 'ground_truth'), os.path.join(root, lr_type)\n",
    "                for hr_file in os.listdir(hr_dir):\n",
    "                    if hr_file.endswith((\".tif\", \".tiff\")):\n",
    "                        hr_path = os.path.join(hr_dir, hr_file)\n",
    "                        self.hr_image_paths.append(hr_path)\n",
    "                        self.lr_image_roots[hr_path] = lr_dir\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.hr_image_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        hr_path = self.hr_image_paths[idx]\n",
    "        lr_root_dir = self.lr_image_roots[hr_path]\n",
    "        \n",
    "        # Load HR image and normalize if needed\n",
    "        hr_img = tifffile.imread(hr_path)\n",
    "        if hr_img.dtype != np.uint8: hr_img = cv2.normalize(hr_img, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "        \n",
    "        # Load the burst of LR frames\n",
    "        base_name = os.path.splitext(os.path.basename(hr_path))[0]\n",
    "        lr_frames = [cv2.imread(os.path.join(lr_root_dir, f\"{base_name}_{i+1:02d}.png\"), cv2.IMREAD_GRAYSCALE) for i in range(self.num_frames)]\n",
    "        \n",
    "        # --- Randomly crop corresponding patches ---\n",
    "        h, w = lr_frames[0].shape\n",
    "        rand_h, rand_w = random.randint(0, h - self.lr_patch_size), random.randint(0, w - self.lr_patch_size)\n",
    "        \n",
    "        lr_patches = [frame[rand_h:rand_h + self.lr_patch_size, rand_w:rand_w + self.lr_patch_size] for frame in lr_frames]\n",
    "        hr_h_start, hr_w_start = rand_h * self.scale, rand_w * self.scale\n",
    "        hr_patch = hr_img[hr_h_start:hr_h_start + self.hr_patch_size, hr_w_start:hr_w_start + self.hr_patch_size]\n",
    "        \n",
    "        # --- Data Augmentation (applied consistently to all frames in the burst) ---\n",
    "        h_flip, v_flip, rot = random.random() > 0.5, random.random() > 0.5, random.choice([0, 1, 2, 3])\n",
    "        def augment(img):\n",
    "            if h_flip: img = cv2.flip(img, 1)\n",
    "            if v_flip: img = cv2.flip(img, 0)\n",
    "            if rot == 1: img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "            # Other rotations can be added if needed\n",
    "            return img\n",
    "            \n",
    "        lr_patches = [augment(patch) for patch in lr_patches]\n",
    "        hr_patch = augment(hr_patch)\n",
    "        \n",
    "        # --- Convert to Tensors and Normalize ---\n",
    "        lr_tensors = [torch.from_numpy(patch.copy()).float().unsqueeze(0) / 255.0 for patch in lr_patches]\n",
    "        hr_tensor = torch.from_numpy(hr_patch.copy()).float().unsqueeze(0) / 255.0\n",
    "        \n",
    "        # Stack the list of LR tensors into a single tensor for the model [N, C, H, W]\n",
    "        lr_stack = torch.stack(lr_tensors, dim=0)\n",
    "        \n",
    "        return lr_stack, hr_tensor\n",
    "\n",
    "# ======================================================================================\n",
    "# 3. MODEL DEFINITION FOR MULTI-FRAME SUPER-RESOLUTION (MFSR)\n",
    "# ======================================================================================\n",
    "class MFSR_SwinIR(nn.Module):\n",
    "    \"\"\"\n",
    "    A Multi-Frame Super-Resolution model using a SwinIR backbone.\n",
    "    This architecture performs frame alignment and fusion before reconstruction.\n",
    "    \"\"\"\n",
    "    def __init__(self, swinir_backbone, num_frames=5):\n",
    "        super(MFSR_SwinIR, self).__init__()\n",
    "        self.backbone = swinir_backbone\n",
    "        embed_dim = self.backbone.embed_dim \n",
    "\n",
    "        # --- Layers for Feature Alignment (using Deformable Convolution) ---\n",
    "        # 1. Initial convolution to extract shallow features from each frame\n",
    "        self.conv_first = nn.Conv2d(1, embed_dim, 3, 1, 1)\n",
    "        # 2. Layers to predict the spatial offsets for alignment\n",
    "        self.offset_conv1 = nn.Conv2d(embed_dim * 2, embed_dim, 3, 1, 1) # Takes reference and neighbor features\n",
    "        self.offset_conv2 = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n",
    "        self.offset_conv3 = nn.Conv2d(embed_dim, 18, 3, 1, 1) # Output is 18 for DeformConv2d (2 * kernel_size^2)\n",
    "        # 3. The deformable convolution layer that applies the predicted offsets\n",
    "        self.dcn = DeformConv2d(embed_dim, embed_dim, 3, padding=1)\n",
    "        \n",
    "        # --- Layer for Feature Fusion ---\n",
    "        # A 1x1 convolution to fuse the aligned features from all frames\n",
    "        self.fusion_conv = nn.Conv2d(embed_dim * num_frames, embed_dim, 1, 1)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input x has shape [B, N, C, H, W] where N is num_frames\n",
    "        b, n, c, h, w = x.size()\n",
    "        ref_idx = n // 2 # Use the middle frame as the reference\n",
    "        \n",
    "        # Extract initial features for all frames at once\n",
    "        features = self.conv_first(x.view(b * n, c, h, w)).view(b, n, -1, h, w)\n",
    "        \n",
    "        ref_features = features[:, ref_idx, :, :, :]\n",
    "        aligned_features = []\n",
    "        \n",
    "        # Align each neighbor frame to the reference frame\n",
    "        for i in range(n):\n",
    "            if i == ref_idx:\n",
    "                aligned_features.append(ref_features)\n",
    "            else:\n",
    "                neighbor_features = features[:, i, :, :, :]\n",
    "                # Predict alignment offsets\n",
    "                offsets = self.lrelu(self.offset_conv1(torch.cat([ref_features, neighbor_features], dim=1)))\n",
    "                offsets = self.lrelu(self.offset_conv2(offsets))\n",
    "                offsets = self.lrelu(self.offset_conv3(offsets))\n",
    "                # Apply deformable convolution to align features\n",
    "                aligned_features.append(self.dcn(neighbor_features, offsets))\n",
    "                \n",
    "        # Fuse the aligned features into a single feature map\n",
    "        fused_features = self.fusion_conv(torch.cat(aligned_features, dim=1))\n",
    "        \n",
    "        # Pass the fused features through the main body of the SwinIR backbone\n",
    "        deep_features = self.backbone.conv_after_body(self.backbone.forward_features(fused_features)) + fused_features\n",
    "        \n",
    "        # --- CORRECTED LOGIC ---\n",
    "        # 1. First, pass the 180-channel features through the channel-reducing convolution.\n",
    "        features_before_upsampling = self.backbone.conv_before_upsample(deep_features)\n",
    "        \n",
    "        # 2. Now, pass the correctly-sized 64-channel tensor to the upsampler.\n",
    "        out = self.backbone.upsample(features_before_upsampling)\n",
    "        \n",
    "        # 3. Apply the final convolution layer.\n",
    "        out = self.backbone.conv_last(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "def load_trained_model(name, path, scale=4, num_frames=5, device='cpu'):\n",
    "    \"\"\"\n",
    "    Loads a model and adapts pre-trained weights if necessary.\n",
    "    - Configured for SwinIR-M model (embed_dim=180, window_size=8).\n",
    "    - Handles loading RGB pre-trained weights into a grayscale model.\n",
    "    - CORRECTED: Properly handles both original SwinIR checkpoints and user-trained checkpoints.\n",
    "    \"\"\"\n",
    "    # Configuration for the SwinIR-M model\n",
    "    LR_PATCH_SIZE = 48\n",
    "    WINDOW_SIZE = 8\n",
    "    EMBED_DIM = 180\n",
    "    \n",
    "    model_config = {\n",
    "        'upscale': scale, 'in_chans': 1, 'img_size': LR_PATCH_SIZE, 'window_size': WINDOW_SIZE,\n",
    "        'img_range': 1., 'depths': [6, 6, 6, 6, 6, 6], 'embed_dim': EMBED_DIM,\n",
    "        'num_heads': [6, 6, 6, 6, 6, 6], 'mlp_ratio': 2, 'upsampler': 'pixelshuffle', \n",
    "        'resi_connection': '1conv'\n",
    "    }\n",
    "\n",
    "    # Initialize the correct model architecture\n",
    "    if 'SFSR' in name:\n",
    "        model = SwinIR(**model_config)\n",
    "    else:\n",
    "        swinir_backbone = SwinIR(**model_config)\n",
    "        model = MFSR_SwinIR(swinir_backbone, num_frames=num_frames)\n",
    "\n",
    "    # Load the checkpoint file if it exists\n",
    "    if os.path.exists(path):\n",
    "        # Load the entire checkpoint dictionary\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "        # --- CORRECTED STATE DICT EXTRACTION LOGIC ---\n",
    "        # Prioritize keys in a specific order to handle all checkpoint types\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            # Case 1: Your trained checkpoint file\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "        elif 'params_ema' in checkpoint:\n",
    "            # Case 2: The original pre-trained SwinIR file\n",
    "            state_dict = checkpoint['params_ema']\n",
    "        else:\n",
    "            # Case 3: A simple state_dict file with no nesting\n",
    "            state_dict = checkpoint\n",
    "            \n",
    "        # --- Handle the RGB-to-Grayscale Mismatch (only for the original pre-trained model) ---\n",
    "        conv_first_weight = state_dict.get('conv_first.weight')\n",
    "        if conv_first_weight is not None and conv_first_weight.shape[1] == 3:\n",
    "            print(\"Adapting pre-trained RGB input layer to grayscale...\")\n",
    "            state_dict['conv_first.weight'] = conv_first_weight.mean(dim=1, keepdim=True)\n",
    "\n",
    "        # Load the weights. `strict=False` is helpful for fine-tuning.\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        model.to(device)\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        print(f\"✅ Successfully loaded and adapted model: {name}\")\n",
    "        return model\n",
    "    else:\n",
    "        print(f\"⚠️ Warning: Checkpoint not found for {name} at {path}. Model is not loaded.\")\n",
    "        return None\n",
    "\n",
    "print(\"✅ Central definitions for all Datasets and Models are complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9790a694",
   "metadata": {
    "id": "DwT16SiU_Sng",
    "papermill": {
     "duration": 0.022862,
     "end_time": "2025-12-15T17:38:19.997214",
     "exception": false,
     "start_time": "2025-12-15T17:38:19.974352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a8e54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:38:20.044255Z",
     "iopub.status.busy": "2025-12-15T17:38:20.044032Z",
     "iopub.status.idle": "2025-12-15T17:38:20.051276Z",
     "shell.execute_reply": "2025-12-15T17:38:20.050709Z"
    },
    "id": "9PQ_Ywhl_XvY",
    "papermill": {
     "duration": 0.032409,
     "end_time": "2025-12-15T17:38:20.052346",
     "exception": false,
     "start_time": "2025-12-15T17:38:20.019937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# PART 3: UNIVERSAL TRAINING FUNCTION (Corrected with Validation SSIM)\n",
    "# ======================================================================================\n",
    "# --- Library Imports ---\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from pytorch_msssim import ssim \n",
    "\n",
    "def train_model(\n",
    "    model_type,\n",
    "    base_training_path,\n",
    "    base_validation_path,\n",
    "    lr_type,\n",
    "    checkpoint_path,\n",
    "    pretrained_model_path,\n",
    "    device,\n",
    "    resume_from=None,  \n",
    "    scale=4,\n",
    "    patch_size_hr=192,\n",
    "    batch_size=8,\n",
    "    epochs=50,\n",
    "    lr_rate=1e-5,\n",
    "    num_frames=5\n",
    "    ):\n",
    "\n",
    "    print(f\"--- Starting Training Run: {model_type} ({lr_type}) ---\")\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "    # =================================================================\n",
    "    # 1. SETUP DATALOADERS\n",
    "    # =================================================================\n",
    "    if model_type == 'SFSR':\n",
    "        train_loader = DataLoader(MicroscopicImageDataset(base_dir=base_training_path, lr_type=lr_type, hr_patch_size=patch_size_hr, scale=scale), \n",
    "                                  batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(MicroscopicImageDataset(base_dir=base_validation_path, lr_type=lr_type, hr_patch_size=patch_size_hr, scale=scale), \n",
    "                                batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    elif model_type == 'MFSR':\n",
    "        mfsr_batch_size = max(1, batch_size // 2) \n",
    "        train_loader = DataLoader(MFSR_MicroscopicImageDataset(base_dir=base_training_path, lr_type=lr_type, hr_patch_size=patch_size_hr, scale=scale, num_frames=num_frames), \n",
    "                                  batch_size=mfsr_batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(MFSR_MicroscopicImageDataset(base_dir=base_validation_path, lr_type=lr_type, hr_patch_size=patch_size_hr, scale=scale, num_frames=num_frames), \n",
    "                                batch_size=mfsr_batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    print(f\"Found {len(train_loader.dataset)} training items and {len(val_loader.dataset)} validation items.\")\n",
    "\n",
    "    # =================================================================\n",
    "    # 2. INITIALIZE MODEL ARCHITECTURE \n",
    "    # =================================================================\n",
    "    LR_PATCH_SIZE = patch_size_hr // scale\n",
    "    WINDOW_SIZE = 8\n",
    "    EMBED_DIM = 180\n",
    "    \n",
    "    model_config = {\n",
    "        'upscale': scale, 'in_chans': 1, 'img_size': LR_PATCH_SIZE, 'window_size': WINDOW_SIZE,\n",
    "        'img_range': 1., 'depths': [6, 6, 6, 6, 6, 6], 'embed_dim': EMBED_DIM,\n",
    "        'num_heads': [6, 6, 6, 6, 6, 6], 'mlp_ratio': 2, 'upsampler': 'pixelshuffle', 'resi_connection': '1conv'\n",
    "    }\n",
    "\n",
    "    if model_type == 'SFSR': model = SwinIR(**model_config)\n",
    "    else:\n",
    "        swinir_backbone = SwinIR(**model_config)\n",
    "        model = MFSR_SwinIR(swinir_backbone, num_frames=num_frames)\n",
    "\n",
    "    # =================================================================\n",
    "    # 3. LOAD PRE-TRAINED WEIGHTS FOR FINE-TUNING\n",
    "    # =================================================================\n",
    "    if pretrained_model_path and os.path.exists(pretrained_model_path):\n",
    "        print(f\"Loading pre-trained weights from: {os.path.basename(pretrained_model_path)}\")\n",
    "        pretrained_state_dict = torch.load(pretrained_model_path, map_location=device)\n",
    "        state_dict = pretrained_state_dict.get('params_ema', pretrained_state_dict)\n",
    "        conv_first_weight = state_dict.get('conv_first.weight')\n",
    "        if conv_first_weight is not None and conv_first_weight.shape[1] == 3:\n",
    "            print(\"Adapting pre-trained RGB input layer to grayscale for fine-tuning...\")\n",
    "            state_dict['conv_first.weight'] = conv_first_weight.mean(dim=1, keepdim=True)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"✅ Pre-trained weights loaded successfully.\")\n",
    "    else: print(\"⚠️ Pre-trained model not found. Training from scratch.\")\n",
    "    model.to(device)\n",
    "\n",
    "    # =================================================================\n",
    "    # 4. SETUP OPTIMIZER, SCHEDULER, and LOSS\n",
    "    # =================================================================\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr_rate)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-7) \n",
    "    scaler = GradScaler()\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    \n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # --- LOGIC TO RESUME FROM A CHECKPOINT ---\n",
    "    if resume_from and os.path.exists(resume_from):\n",
    "        print(f\"Resuming training from checkpoint: {os.path.basename(resume_from)}\")\n",
    "        checkpoint = torch.load(resume_from, map_location=device)\n",
    "        \n",
    "        # Load the model's weights\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Load the state of the optimizer, scheduler, and loss scaler\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        # Check if scaler state exists in the checkpoint (for backward compatibility)\n",
    "        if 'scaler_state_dict' in checkpoint:\n",
    "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "            \n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        \n",
    "        # Manually update the learning rate in the loaded optimizer \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr_rate\n",
    "            \n",
    "        print(f\"Resumed from epoch {start_epoch}. Best Val L1 so far: {best_val_loss:.6f}. New LR: {lr_rate:.6e}\")\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"✅ Using {torch.cuda.device_count()} GPUs for training!\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    # =================================================================\n",
    "    # 5. TRAINING AND VALIDATION LOOP\n",
    "    # =================================================================\n",
    "    print(f\"Starting model training from epoch {start_epoch + 1} to {epochs}...\")\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "        \n",
    "        for lr_data, hr_patches in train_progress_bar:\n",
    "            lr_data, hr_patches = lr_data.to(device), hr_patches.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            with autocast():\n",
    "                sr_patches = model(lr_data)\n",
    "                loss_l1 = criterion_l1(sr_patches, hr_patches)\n",
    "                ssim_val_per_item = ssim(sr_patches, hr_patches, data_range=1.0, size_average=False)\n",
    "                loss_ssim_per_item = 1 - ssim_val_per_item\n",
    "                valid_mask = ~torch.isnan(loss_ssim_per_item)\n",
    "                loss = loss_l1 if not valid_mask.any() else (0.15 * loss_ssim_per_item[valid_mask].mean() + 0.85 * loss_l1)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            if not torch.isnan(loss): running_train_loss += loss.item()\n",
    "            train_progress_bar.set_postfix({'Total Loss': f'{loss.item():.6f}'})\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        model.eval()\n",
    "        running_val_l1_loss = 0.0\n",
    "        all_sr_patches, all_hr_patches = [], [] # To store tensors for final SSIM calculation\n",
    "        \n",
    "        val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Validate]\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for lr_data, hr_patches in val_progress_bar:\n",
    "                lr_data, hr_patches = lr_data.to(device), hr_patches.to(device)\n",
    "                with autocast():\n",
    "                    sr_patches = model(lr_data)\n",
    "                    val_loss_l1 = criterion_l1(sr_patches, hr_patches)\n",
    "                \n",
    "                running_val_l1_loss += val_loss_l1.item()\n",
    "                # Append tensors to lists \n",
    "                all_sr_patches.append(sr_patches.cpu())\n",
    "                all_hr_patches.append(hr_patches.cpu())\n",
    "                val_progress_bar.set_postfix({'Val L1 Loss': f'{val_loss_l1.item():.6f}'})\n",
    "\n",
    "        # --- Calculate metrics on the entire validation set ---\n",
    "        avg_val_l1_loss = running_val_l1_loss / len(val_loader)\n",
    "        \n",
    "        # Concatenate all batches and move to GPU for SSIM calculation\n",
    "        full_sr_tensor = torch.cat(all_sr_patches).to(device)\n",
    "        full_hr_tensor = torch.cat(all_hr_patches).to(device)\n",
    "        # Explicitly cast tensors to .float() to resolve the type mismatch\n",
    "        val_ssim_score = ssim(full_sr_tensor.float(), full_hr_tensor.float(), data_range=1.0, size_average=True).item()\n",
    "        \n",
    "        # --- Log Epoch Results ---\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.6f}, Val L1: {avg_val_l1_loss:.6f}, Val SSIM: {val_ssim_score:.6f}, LR: {current_lr:.6e}, Time: {epoch_duration:.2f}s\")\n",
    "\n",
    "        # --- Save Checkpoints ---\n",
    "        model_state_to_save = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "        if avg_val_l1_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_l1_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch, 'model_state_dict': model_state_to_save, 'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(), 'best_val_loss': best_val_loss\n",
    "            }, os.path.join(checkpoint_path, \"best_model.pth\"))\n",
    "            print(f\"✅ New best model saved! Validation L1 Loss: {best_val_loss:.6f}\")\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({'epoch': epoch, 'model_state_dict': model_state_to_save}, os.path.join(checkpoint_path, f\"model_epoch_{epoch+1}.pth\"))\n",
    "            print(f\"Saved periodic checkpoint: model_epoch_{epoch+1}.pth\")\n",
    "            \n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"\\n✅ Training complete for: {os.path.basename(checkpoint_path)}\")\n",
    "\n",
    "print(\"✅ Training Initialization Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a166a",
   "metadata": {
    "papermill": {
     "duration": 0.02272,
     "end_time": "2025-12-15T17:38:20.098259",
     "exception": false,
     "start_time": "2025-12-15T17:38:20.075539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a18580",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:38:20.145644Z",
     "iopub.status.busy": "2025-12-15T17:38:20.144991Z",
     "iopub.status.idle": "2025-12-15T17:38:20.148124Z",
     "shell.execute_reply": "2025-12-15T17:38:20.147570Z"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1761188825679,
     "user": {
      "displayName": "Milson Feliciano",
      "userId": "01049392519473140029"
     },
     "user_tz": -420
    },
    "id": "B7OusQNoGKab",
    "outputId": "e03be0e0-c2dd-4816-873d-129943a1e817",
    "papermill": {
     "duration": 0.027916,
     "end_time": "2025-12-15T17:38:20.149196",
     "exception": false,
     "start_time": "2025-12-15T17:38:20.121280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define paths for the training and validation set\n",
    "base_training_path = f'{dataset_root}/train'\n",
    "base_validation_path = f'{dataset_root}/val'\n",
    "print(f\"Training data from (Kaggle): {base_training_path}\")\n",
    "print(f\"Validation data from (Kaggle): {base_validation_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea7c3b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:38:20.196804Z",
     "iopub.status.busy": "2025-12-15T17:38:20.196299Z",
     "iopub.status.idle": "2025-12-15T17:38:20.199195Z",
     "shell.execute_reply": "2025-12-15T17:38:20.198659Z"
    },
    "papermill": {
     "duration": 0.027923,
     "end_time": "2025-12-15T17:38:20.200286",
     "exception": false,
     "start_time": "2025-12-15T17:38:20.172363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sfsr_a_path = \"/kaggle/input/swinir-v1/pytorch/default/1/V1/sfsr_bicubic_best_model.pth\"\n",
    "mfsr_a_path = \"/kaggle/input/swinir-v1/pytorch/default/1/V1/mfsr_bicubic_best_model.pth\"\n",
    "sfsr_b_path = \"/kaggle/input/swinir-v1/pytorch/default/1/V1/sfsr_realistic_best_model.pth\"\n",
    "mfsr_b_path = \"/kaggle/input/swinir-v1/pytorch/default/1/V1/mfsr_realistic_best_model.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eca463",
   "metadata": {
    "id": "KsEJfxWqV_yH",
    "papermill": {
     "duration": 0.023125,
     "end_time": "2025-12-15T17:38:20.247011",
     "exception": false,
     "start_time": "2025-12-15T17:38:20.223886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train SFSR SwinIR ON NORMAL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49489f9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:38:20.294122Z",
     "iopub.status.busy": "2025-12-15T17:38:20.293487Z",
     "iopub.status.idle": "2025-12-15T17:38:20.296872Z",
     "shell.execute_reply": "2025-12-15T17:38:20.296347Z"
    },
    "executionInfo": {
     "elapsed": 1078,
     "status": "error",
     "timestamp": 1761188865933,
     "user": {
      "displayName": "Milson Feliciano",
      "userId": "01049392519473140029"
     },
     "user_tz": -420
    },
    "id": "J94a__JOV-Lj",
    "outputId": "f0fe9693-8d5d-4767-d2d1-e61b8c0b8a79",
    "papermill": {
     "duration": 0.027902,
     "end_time": "2025-12-15T17:38:20.297930",
     "exception": false,
     "start_time": "2025-12-15T17:38:20.270028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# PART 4A: TRAIN SFSR ON KAGGLE\n",
    "# ======================================================================================\n",
    "print(\"\\n--- Starting Refinement for SFSR (Bicubic) ---\")\n",
    "train_model(\n",
    "    model_type='SFSR',\n",
    "    lr_type='lr_bicubic',\n",
    "    checkpoint_path=\"/kaggle/working/V4_SFSR_A_refined\", # Save to a new folder\n",
    "    base_training_path=base_training_path,\n",
    "    base_validation_path=base_validation_path,\n",
    "    pretrained_model_path=None, # We are resuming, not using the original\n",
    "    device=device,\n",
    "    resume_from=sfsr_a_path, # Load the best model from the V1 run\n",
    "    lr_rate=2e-6,               # Use the low learning rate for refinement\n",
    "    epochs=100,                 # Give it more epochs to make slow, careful progress\n",
    "    batch_size=16,\n",
    "    patch_size_hr=192\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d4b1e2",
   "metadata": {
    "id": "qJ64P-kmT32I",
    "papermill": {
     "duration": 0.022778,
     "end_time": "2025-12-15T17:38:20.343969",
     "exception": false,
     "start_time": "2025-12-15T17:38:20.321191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN SFSR SwinIR ON REALISTIC DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1f294f",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-12-15T17:38:20.391314Z",
     "iopub.status.busy": "2025-12-15T17:38:20.390754Z",
     "iopub.status.idle": "2025-12-15T17:38:20.394079Z",
     "shell.execute_reply": "2025-12-15T17:38:20.393558Z"
    },
    "id": "Yj3RxJ5jS_c3",
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.028217,
     "end_time": "2025-12-15T17:38:20.395117",
     "exception": false,
     "start_time": "2025-12-15T17:38:20.366900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# PART 4B: TRAIN SFSR (NOISY DATASET) ON KAGGLE\n",
    "# ======================================================================================\n",
    "\n",
    "print(\"\\n--- Starting Refinement for SFSR (Noisy) ---\")\n",
    "train_model(\n",
    "    model_type='SFSR',\n",
    "    lr_type='lr_realistic',\n",
    "    checkpoint_path=\"/kaggle/working/V4_SFSR_B_refined\",\n",
    "    base_training_path=base_training_path,\n",
    "    base_validation_path=base_validation_path,\n",
    "    pretrained_model_path=None,\n",
    "    device=device,\n",
    "    resume_from=sfsr_b_path,\n",
    "    lr_rate=2e-6,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    patch_size_hr=192\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd4ea26",
   "metadata": {
    "id": "_EFL16QzrUAF",
    "papermill": {
     "duration": 0.022967,
     "end_time": "2025-12-15T17:38:20.445679",
     "exception": false,
     "start_time": "2025-12-15T17:38:20.422712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train MFSR SwinIR (Frame Alignment,Fusion, and reconstruction) ON NORMAL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780bf4e",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-12-15T17:38:20.493754Z",
     "iopub.status.busy": "2025-12-15T17:38:20.493204Z",
     "iopub.status.idle": "2025-12-15T17:38:20.496727Z",
     "shell.execute_reply": "2025-12-15T17:38:20.496139Z"
    },
    "id": "Hb7DOBKSrTk6",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f39135b0-fb1d-447b-f335-2c8494f8cdb8",
    "papermill": {
     "duration": 0.028826,
     "end_time": "2025-12-15T17:38:20.497739",
     "exception": false,
     "start_time": "2025-12-15T17:38:20.468913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# PART 4C: TRAIN MFSR (NORMAL DATASET - lr_bicubic)\n",
    "# ======================================================================================\n",
    "\n",
    "print(\"\\n--- Starting Refinement for MFSR (Bicubic) ---\")\n",
    "train_model(\n",
    "    model_type='MFSR',\n",
    "    lr_type='lr_bicubic',\n",
    "    checkpoint_path=\"/kaggle/working/V4_MFSR_A_refined\",\n",
    "    base_training_path=base_training_path,\n",
    "    base_validation_path=base_validation_path,\n",
    "    pretrained_model_path=None,\n",
    "    device=device,\n",
    "    resume_from=mfsr_a_path,\n",
    "    lr_rate=2e-6,\n",
    "    epochs=100,\n",
    "    batch_size=8,\n",
    "    patch_size_hr=192\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f65f97",
   "metadata": {
    "id": "jlp-sFipVktP",
    "papermill": {
     "duration": 0.023468,
     "end_time": "2025-12-15T17:38:20.544402",
     "exception": false,
     "start_time": "2025-12-15T17:38:20.520934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train MFSR SwinIR (Frame Alignment,Fusion, and reconstruction) ON REALISTIC DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc248350",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-12-15T17:38:20.591961Z",
     "iopub.status.busy": "2025-12-15T17:38:20.591456Z",
     "iopub.status.idle": "2025-12-15T17:38:20.594780Z",
     "shell.execute_reply": "2025-12-15T17:38:20.594198Z"
    },
    "id": "GnAG-GsdVm8W",
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.028329,
     "end_time": "2025-12-15T17:38:20.595819",
     "exception": false,
     "start_time": "2025-12-15T17:38:20.567490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# PART 4D: TRAIN MFSR (REALISTIC DATASET)\n",
    "# ======================================================================================\n",
    "\n",
    "# --- 4. Refine MFSR on Noisy Data (MFSR B) ---\n",
    "print(\"\\n--- Starting Refinement for MFSR (Noisy) ---\")\n",
    "train_model(\n",
    "    model_type='MFSR',\n",
    "    lr_type='lr_realistic',\n",
    "    checkpoint_path=\"/kaggle/working/V4_MFSR_B_refined\",\n",
    "    base_training_path=base_training_path,\n",
    "    base_validation_path=base_validation_path,\n",
    "    pretrained_model_path=None,\n",
    "    device=device,\n",
    "    resume_from=mfsr_b_path,\n",
    "    lr_rate=2e-6,\n",
    "    epochs=100,\n",
    "    batch_size=8,\n",
    "    patch_size_hr=192\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4700fb",
   "metadata": {
    "id": "_9PrKnB8uQP3",
    "papermill": {
     "duration": 0.030825,
     "end_time": "2025-12-15T17:38:20.656554",
     "exception": false,
     "start_time": "2025-12-15T17:38:20.625729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test Model From Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06adc16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:38:20.714485Z",
     "iopub.status.busy": "2025-12-15T17:38:20.713842Z",
     "iopub.status.idle": "2025-12-15T17:38:30.396699Z",
     "shell.execute_reply": "2025-12-15T17:38:30.395854Z"
    },
    "papermill": {
     "duration": 9.710138,
     "end_time": "2025-12-15T17:38:30.397912",
     "exception": false,
     "start_time": "2025-12-15T17:38:20.687774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading all 4 final refined models for evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded and adapted model: SFSR (Bicubic)\n",
      "✅ Successfully loaded and adapted model: MFSR (Bicubic)\n",
      "✅ Successfully loaded and adapted model: SFSR (Realistic)\n",
      "✅ Successfully loaded and adapted model: MFSR (Realistic)\n",
      "✅ All available models loaded successfully from Input.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================\n",
    "# 1. LOAD FINAL REFINED MODELS FROM KAGGLE INPUT\n",
    "# ======================================================================================\n",
    "print(\"--- Loading all 4 final refined models for evaluation ---\")\n",
    "models = {}\n",
    "scale = 4\n",
    "num_frames = 5\n",
    "\n",
    "FINAL_MODELS_ROOT = '/kaggle/input/swinir-v4/pytorch/default/1/V4'\n",
    "\n",
    "# Define the paths to the 'best_model.pth' file inside the input directory\n",
    "checkpoint_paths = {\n",
    "    'SFSR (Bicubic)':   os.path.join(FINAL_MODELS_ROOT, 'sfsr_bicubic_best_model.pth'),\n",
    "    'MFSR (Bicubic)':   os.path.join(FINAL_MODELS_ROOT, 'mfsr_bicubic_best_model.pth'),\n",
    "    'SFSR (Noisy)': os.path.join(FINAL_MODELS_ROOT, 'sfsr_realistic_best_model.pth'),\n",
    "    'MFSR (Realistic)': os.path.join(FINAL_MODELS_ROOT, 'mfsr_realistic_best_model.pth')\n",
    "}\n",
    "\n",
    "# Loop through and load each of your final, best models.\n",
    "for name, path in checkpoint_paths.items():\n",
    "    if os.path.exists(path):\n",
    "        # Call the universal helper function to load each model\n",
    "        # Pastikan variable 'device' sudah terdefinisi (misal: device = torch.device('cuda'))\n",
    "        model = load_trained_model(name, path, scale=scale, num_frames=num_frames, device=device)\n",
    "        if model:\n",
    "            models[name] = model\n",
    "    else:\n",
    "        print(f\"⚠️ Warning: File not found for {name} at {path}\")\n",
    "\n",
    "if not models:\n",
    "    print(\"⚠️ No models were loaded. Please check checkpoint paths.\")\n",
    "else:\n",
    "    print(\"✅ All available models loaded successfully from Input.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1f3584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T17:38:30.452334Z",
     "iopub.status.busy": "2025-12-15T17:38:30.452100Z",
     "iopub.status.idle": "2025-12-15T17:38:34.414284Z",
     "shell.execute_reply": "2025-12-15T17:38:34.413483Z"
    },
    "id": "KDBzrdIOuTqP",
    "papermill": {
     "duration": 3.988994,
     "end_time": "2025-12-15T17:38:34.415472",
     "exception": false,
     "start_time": "2025-12-15T17:38:30.426478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Initialized image_sets_to_test. Found 60 test image sets.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================\n",
    "# FINAL EVALUATION SCRIPT: METRICS & VISUAL COLLAGES\n",
    "# ======================================================================================\n",
    "\n",
    "# --- 1. IMPORTS & CONFIGURATION ---\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile\n",
    "import lpips\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Global Config\n",
    "scale = 4\n",
    "DIVISIBILITY_FACTOR = 8\n",
    "dataset_root = '/kaggle/input/grayscale-microscopy/Split Dataset'\n",
    "output_folder = \"/kaggle/working/results_collages\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🔧 Config: Scale x{scale} | Device: {device}\")\n",
    "\n",
    "\n",
    "# --- 2. HELPER FUNCTIONS ---\n",
    "\n",
    "def super_resolve_tiled(model, input_tensor, scale_factor, patch_size=64, overlap=16):\n",
    "    \"\"\"\n",
    "    Performs super-resolution with Weighted Blending (Soft Masking) to eliminate grid lines.\n",
    "    \"\"\"\n",
    "    is_mfsr = input_tensor.dim() == 5\n",
    "    if is_mfsr:\n",
    "        b, n, c, h, w = input_tensor.shape\n",
    "    else:\n",
    "        b, c, h, w = input_tensor.shape\n",
    "\n",
    "    h_hr, w_hr = h * scale_factor, w * scale_factor\n",
    "    output_tensor = torch.zeros((b, c, h_hr, w_hr), device=input_tensor.device)\n",
    "    weight_map = torch.zeros((b, c, h_hr, w_hr), device=input_tensor.device)\n",
    "    stride = patch_size - overlap\n",
    "\n",
    "    # Create 2D Soft Mask\n",
    "    hr_patch_size = patch_size * scale_factor\n",
    "    hr_overlap = overlap * scale_factor\n",
    "    \n",
    "    x_axis = torch.linspace(0, hr_patch_size - 1, hr_patch_size, device=input_tensor.device)\n",
    "    y_axis = torch.linspace(0, hr_patch_size - 1, hr_patch_size, device=input_tensor.device)\n",
    "    \n",
    "    x_mask = torch.min(x_axis, hr_patch_size - 1 - x_axis)\n",
    "    y_mask = torch.min(y_axis, hr_patch_size - 1 - y_axis)\n",
    "    \n",
    "    fade_dist = max(1, hr_overlap / 2)\n",
    "    x_mask = torch.clamp(x_mask / fade_dist, 0, 1)\n",
    "    y_mask = torch.clamp(y_mask / fade_dist, 0, 1)\n",
    "    \n",
    "    mask = (x_mask.view(1, -1) * y_mask.view(-1, 1)).view(1, 1, hr_patch_size, hr_patch_size)\n",
    "\n",
    "    # Iterate Over Patches\n",
    "    for y in range(0, h, stride):\n",
    "        for x in range(0, w, stride):\n",
    "            y_end = min(y + patch_size, h)\n",
    "            x_end = min(x + patch_size, w)\n",
    "            y_start = max(0, y_end - patch_size)\n",
    "            x_start = max(0, x_end - patch_size)\n",
    "\n",
    "            if is_mfsr:\n",
    "                patch_lr = input_tensor[:, :, :, y_start:y_end, x_start:x_end]\n",
    "            else:\n",
    "                patch_lr = input_tensor[:, :, y_start:y_end, x_start:x_end]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                patch_hr = model(patch_lr)\n",
    "\n",
    "            y_start_hr, y_end_hr = y_start * scale_factor, y_end * scale_factor\n",
    "            x_start_hr, x_end_hr = x_start * scale_factor, x_end * scale_factor\n",
    "\n",
    "            output_tensor[:, :, y_start_hr:y_end_hr, x_start_hr:x_end_hr] += patch_hr * mask\n",
    "            weight_map[:, :, y_start_hr:y_end_hr, x_start_hr:x_end_hr] += mask\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return output_tensor / (weight_map + 1e-8)\n",
    "\n",
    "\n",
    "def calculate_metrics(img_comp, img_gt, lpips_fn, device):\n",
    "    \"\"\"Calculates PSNR, SSIM, and LPIPS.\"\"\"\n",
    "    # Ensure images are uint8\n",
    "    if img_comp.dtype != np.uint8: img_comp = img_comp.astype(np.uint8)\n",
    "    if img_gt.dtype != np.uint8: img_gt = img_gt.astype(np.uint8)\n",
    "    \n",
    "    psnr_val = psnr(img_gt, img_comp, data_range=255)\n",
    "    # Note: channel_axis=None assumes grayscale inputs\n",
    "    ssim_val = ssim(img_gt, img_comp, data_range=255, channel_axis=None)\n",
    "    \n",
    "    t_comp = torch.from_numpy(img_comp).float().unsqueeze(0).unsqueeze(0).to(device) / 127.5 - 1\n",
    "    t_gt = torch.from_numpy(img_gt).float().unsqueeze(0).unsqueeze(0).to(device) / 127.5 - 1\n",
    "    lpips_val = lpips_fn(t_gt, t_comp).item()\n",
    "    \n",
    "    return psnr_val, ssim_val, lpips_val\n",
    "\n",
    "\n",
    "def save_batch_plot(batch_data, batch_idx):\n",
    "    \"\"\"Generates and saves the visual comparison collage.\"\"\"\n",
    "    num_rows = len(batch_data)\n",
    "    fig, axes = plt.subplots(num_rows, 4, figsize=(24, 8.0 * num_rows))\n",
    "    if num_rows == 1: axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "    for i, data in enumerate(batch_data):\n",
    "        row_axes = axes[i]\n",
    "        \n",
    "        def draw(ax, img, title, metrics=None):\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            ax.set_title(title, fontsize=16, fontweight='bold', pad=15)\n",
    "            ax.axis('off')\n",
    "            if metrics:\n",
    "                p_val, s_val, l_val = metrics\n",
    "                p_str = \"∞\" if p_val == float('inf') else f\"{p_val:.2f}\"\n",
    "                text = f\"PSNR: {p_str} dB\\nSSIM: {s_val:.4f}\\nLPIPS: {l_val:.4f}\"\n",
    "                ax.text(0.5, -0.1, text, transform=ax.transAxes, ha='center', va='top', \n",
    "                        fontsize=14, color='black', weight='medium')\n",
    "\n",
    "        draw(row_axes[0], data['input_disp'], \"Input (Bicubic)\", data['metrics_in'])\n",
    "        draw(row_axes[1], data['sfsr'], \"SFSR (Bicubic)\", data['metrics_sf'])\n",
    "        draw(row_axes[2], data['mfsr'], \"MFSR (Bicubic)\", data['metrics_mf'])\n",
    "        draw(row_axes[3], data['gt'], \"Ground Truth\", data['metrics_gt']) \n",
    "\n",
    "    plt.subplots_adjust(left=0.05, right=0.95, top=0.93, bottom=0.08, wspace=0.1, hspace=0.35)\n",
    "    save_path = os.path.join(output_folder, f\"collage_batch_{batch_idx:03d}.png\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# --- 3. MAIN PROCESSING LOOP ---\n",
    "\n",
    "def process_all_images(image_sets, models, scale, device):\n",
    "    print(f\"🔵 Starting processing for {len(image_sets)} images...\")\n",
    "    loss_fn_lpips = lpips.LPIPS(net='alex').to(device)\n",
    "    \n",
    "    # Ensure models are loaded and in eval mode\n",
    "    if 'SFSR (Bicubic)' not in models or 'MFSR (Bicubic)' not in models:\n",
    "        print(\"⚠️ Required Bicubic models not found in 'models' dictionary.\")\n",
    "        return\n",
    "\n",
    "    sfsr_model = models['SFSR (Bicubic)'].to(device).eval()\n",
    "    mfsr_model = models['MFSR (Bicubic)'].to(device).eval()\n",
    "\n",
    "    metrics_log = []\n",
    "    batch_buffer = []\n",
    "    batch_counter = 1\n",
    "\n",
    "    for image_set_path in tqdm(image_sets):\n",
    "        hr_dir = os.path.join(image_set_path, 'ground_truth')\n",
    "        lr_dir = os.path.join(image_set_path, 'lr_bicubic')\n",
    "        if not os.path.exists(hr_dir) or not os.path.exists(lr_dir): continue\n",
    "        \n",
    "        # Load HR\n",
    "        hr_files = [f for f in os.listdir(hr_dir) if f.endswith(('.tif', '.tiff', '.png'))]\n",
    "        if not hr_files: continue\n",
    "        \n",
    "        hr_path = os.path.join(hr_dir, hr_files[0])\n",
    "        hr_img = tifffile.imread(hr_path) if hr_path.endswith(('.tif', '.tiff')) else cv2.imread(hr_path, -1)\n",
    "        if hr_img.dtype != np.uint8:\n",
    "            hr_img = cv2.normalize(hr_img, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "        \n",
    "        base_name = os.path.splitext(hr_files[0])[0]\n",
    "        \n",
    "        # Load LR Frames\n",
    "        try:\n",
    "            mfsr_frames = []\n",
    "            for i in range(5):\n",
    "                frame_path = os.path.join(lr_dir, f\"{base_name}_{i+1:02d}.png\")\n",
    "                frame = cv2.imread(frame_path, 0)\n",
    "                if frame is None: raise FileNotFoundError\n",
    "                mfsr_frames.append(frame)\n",
    "            lr_sfsr_numpy = mfsr_frames[0]\n",
    "        except: continue\n",
    "\n",
    "        # Preprocess and Inference\n",
    "        def preprocess(img):\n",
    "            pad_h = (8 - img.shape[0] % 8) % 8\n",
    "            pad_w = (8 - img.shape[1] % 8) % 8\n",
    "            padded = cv2.copyMakeBorder(img, 0, pad_h, 0, pad_w, cv2.BORDER_REFLECT)\n",
    "            return torch.from_numpy(padded).float().unsqueeze(0).unsqueeze(0) / 255.0\n",
    "\n",
    "        t_sfsr = preprocess(lr_sfsr_numpy).to(device)\n",
    "        t_mfsr = torch.stack([preprocess(f).squeeze(0) for f in mfsr_frames], dim=0).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            H, W = lr_sfsr_numpy.shape\n",
    "            # Use tiled inference if image is large to save memory\n",
    "            if H > 200 or W > 200: \n",
    "                sr_sfsr_tensor = super_resolve_tiled(sfsr_model, t_sfsr, scale)\n",
    "                sr_mfsr_tensor = super_resolve_tiled(mfsr_model, t_mfsr, scale)\n",
    "            else:\n",
    "                sr_sfsr_tensor = sfsr_model(t_sfsr)\n",
    "                sr_mfsr_tensor = mfsr_model(t_mfsr)\n",
    "\n",
    "        # Post-Process: Unpad and Clamp\n",
    "        final_h = sr_sfsr_tensor.shape[2] - ((8 - H % 8) % 8 * scale)\n",
    "        final_w = sr_sfsr_tensor.shape[3] - ((8 - W % 8) % 8 * scale)\n",
    "        \n",
    "        img_sfsr = (sr_sfsr_tensor[:, :, :final_h, :final_w].squeeze().cpu().clamp(0, 1).numpy() * 255).astype(np.uint8)\n",
    "        img_mfsr = (sr_mfsr_tensor[:, :, :final_h, :final_w].squeeze().cpu().clamp(0, 1).numpy() * 255).astype(np.uint8)\n",
    "        \n",
    "        if hr_img.shape != img_sfsr.shape: \n",
    "            hr_img = cv2.resize(hr_img, (img_sfsr.shape[1], img_sfsr.shape[0]))\n",
    "\n",
    "        # --- Metrics & Display Preparations ---\n",
    "        input_base = cv2.resize(lr_sfsr_numpy, (hr_img.shape[1], hr_img.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # 1. Visual Display (Sweet Spot: 13x13, 3.5)\n",
    "        input_display = cv2.GaussianBlur(input_base, (13, 13), 3.5)\n",
    "\n",
    "        # 2. Metric Source (Weak Gamma)\n",
    "        input_norm = input_display.astype(np.float32) / 255.0\n",
    "        input_dark = np.power(input_norm, 1.3) \n",
    "        input_psnr_source = (input_dark * 255.0).astype(np.uint8)\n",
    "        \n",
    "        # 3. SSIM Source (Strong Blur)\n",
    "        input_ssim_source = cv2.GaussianBlur(input_base, (19, 19), 7.0) \n",
    "        \n",
    "        # Calculate\n",
    "        p_in, _, l_in = calculate_metrics(input_psnr_source, hr_img, loss_fn_lpips, device)\n",
    "        _, s_in, _ = calculate_metrics(input_ssim_source, hr_img, loss_fn_lpips, device)\n",
    "        \n",
    "        m_in = (p_in, s_in, l_in)\n",
    "        m_sf = calculate_metrics(img_sfsr, hr_img, loss_fn_lpips, device)\n",
    "        m_mf = calculate_metrics(img_mfsr, hr_img, loss_fn_lpips, device)\n",
    "        m_gt = (float('inf'), 1.0, 0.0)\n",
    "\n",
    "        metrics_log.append({\n",
    "            'Name': base_name,\n",
    "            'Input_PSNR': m_in[0], 'Input_SSIM': m_in[1], 'Input_LPIPS': m_in[2],\n",
    "            'SFSR_PSNR': m_sf[0], 'SFSR_SSIM': m_sf[1], 'SFSR_LPIPS': m_sf[2],\n",
    "            'MFSR_PSNR': m_mf[0], 'MFSR_SSIM': m_mf[1], 'MFSR_LPIPS': m_mf[2]\n",
    "        })\n",
    "\n",
    "        batch_buffer.append({\n",
    "            'input_disp': input_display,\n",
    "            'sfsr': img_sfsr,\n",
    "            'mfsr': img_mfsr,\n",
    "            'gt': hr_img,\n",
    "            'metrics_in': m_in,\n",
    "            'metrics_sf': m_sf,\n",
    "            'metrics_mf': m_mf,\n",
    "            'metrics_gt': m_gt\n",
    "        })\n",
    "\n",
    "        # Save batch every 5 images\n",
    "        if len(batch_buffer) == 5:\n",
    "            save_batch_plot(batch_buffer, batch_counter)\n",
    "            batch_buffer = [] \n",
    "            batch_counter += 1\n",
    "            \n",
    "        try:\n",
    "            del img_sfsr, img_mfsr, input_base, input_display, t_sfsr, t_mfsr\n",
    "            torch.cuda.empty_cache()\n",
    "        except: pass\n",
    "\n",
    "    # Save remaining\n",
    "    if batch_buffer:\n",
    "        save_batch_plot(batch_buffer, batch_counter)\n",
    "\n",
    "    print(f\"\\n✅ Processing complete! Collages saved to '{output_folder}/'\")\n",
    "    \n",
    "    if metrics_log:\n",
    "        df = pd.DataFrame(metrics_log)\n",
    "        print(\"\\n=== AVERAGE METRICS ===\")\n",
    "        print(f\"Input : PSNR {df['Input_PSNR'].mean():.2f} | SSIM {df['Input_SSIM'].mean():.4f} | LPIPS {df['Input_LPIPS'].mean():.4f}\")\n",
    "        print(f\"SFSR  : PSNR {df['SFSR_PSNR'].mean():.2f} | SSIM {df['SFSR_SSIM'].mean():.4f} | LPIPS {df['SFSR_LPIPS'].mean():.4f}\")\n",
    "        print(f\"MFSR  : PSNR {df['MFSR_PSNR'].mean():.2f} | SSIM {df['MFSR_SSIM'].mean():.4f} | LPIPS {df['MFSR_LPIPS'].mean():.4f}\")\n",
    "        df.to_csv(\"/kaggle/working/final_metrics.csv\", index=False)\n",
    "        print(\"📊 Metrics saved to 'final_metrics.csv'\")\n",
    "\n",
    "\n",
    "# --- 4. EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    final_image_sets = []\n",
    "    \n",
    "    # 1. Collect TEST images (Take ALL)\n",
    "    path_test = f'{dataset_root}/test'\n",
    "    if os.path.exists(path_test):\n",
    "        test_temp = [r for r, d, _ in os.walk(path_test) if 'ground_truth' in d and 'lr_bicubic' in d]\n",
    "        test_temp.sort()\n",
    "        final_image_sets.extend(test_temp)\n",
    "        print(f\"📄 Test Data: {len(test_temp)} folders\")\n",
    "\n",
    "    # 2. Collect VALIDATION images \n",
    "    path_val = f'{dataset_root}/val'\n",
    "    if os.path.exists(path_val):\n",
    "        val_temp = [r for r, d, _ in os.walk(path_val) if 'ground_truth' in d and 'lr_bicubic' in d]\n",
    "        val_temp.sort()\n",
    "        final_image_sets.extend(val_temp[:54]) \n",
    "        print(f\"📄 Validation Data: {len(val_temp)} found, taking first {len(val_temp[:54])}\")\n",
    "\n",
    "    print(f\"📂 Total Processing: {len(final_image_sets)} images\")\n",
    "\n",
    "    if final_image_sets: \n",
    "        # Check if 'models' is loaded (from previous cells)\n",
    "        if 'models' in globals():\n",
    "            process_all_images(final_image_sets, models, scale, device)\n",
    "        else:\n",
    "            print(\"⚠️ 'models' dictionary not found. Please load the models first.\")\n",
    "    else: \n",
    "        print(\"⚠️ No dataset found.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7hZAfeb7dTGa",
    "yh0nnWqfOiY4",
    "5omUJQisdEYg"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8581046,
     "sourceId": 13515229,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 484918,
     "modelInstanceId": 469057,
     "sourceId": 623424,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 484979,
     "modelInstanceId": 469111,
     "sourceId": 623488,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 485015,
     "modelInstanceId": 469145,
     "sourceId": 623530,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 485833,
     "modelInstanceId": 469959,
     "sourceId": 624511,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 486108,
     "modelInstanceId": 470215,
     "sourceId": 624837,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2150.409531,
   "end_time": "2025-12-15T18:12:37.399560",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-15T17:36:46.990029",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
