{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3c3675",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ba1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. IMPORT LIBRARIES\n",
    "import os\n",
    "import cv2\n",
    "import tifffile\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from skimage.util import random_noise\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully.\")\n",
    "\n",
    "# 2. CONFIGURE LOCAL GOOGLE DRIVE PATH\n",
    "drive_path = \"G:/My Drive/\" \n",
    "\n",
    "# --- Define the specific dataset paths using the base drive_path ---\n",
    "source_dir = os.path.join(drive_path, \"Dataset/BioSR/Training Dataset\")\n",
    "destination_dir = os.path.join(drive_path, \"Dataset/BioSR/Split Dataset\")\n",
    "\n",
    "# Verify that the path exists\n",
    "if not os.path.exists(source_dir):\n",
    "    print(f\"‚ö†Ô∏è ERROR: The specified source directory does not exist: {source_dir}\")\n",
    "    print(\"Please update the 'drive_path' variable with the correct path to your synced Google Drive folder.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Successfully located dataset at: {source_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979771d",
   "metadata": {},
   "source": [
    "## Generate LR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9e3886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning for ground truth images in: C:/Users/milso/Documents/THESIS/Dataset/Split Dataset\n",
      "‚úÖ Found 1092 ground truth images to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating LR Frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1092/1092 [39:41<00:00,  2.18s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================\n",
      "‚úÖ LR Dataset Generation Complete\n",
      "-----------------------------------------\n",
      "Total Ground Truth Images Found: 1092\n",
      "   Successfully Generated: 1092\n",
      "   Failed (Errors):        0\n",
      "=========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. CONFIGURE YOUR LOCAL PATH\n",
    "drive_path = \"C:/Users/milso/Documents/THESIS/\"\n",
    "source_dir = os.path.join(drive_path, \"Dataset/Split Dataset\")\n",
    "\n",
    "# 2. GENERATE LOW-RESOLUTION IMAGE DATASETS\n",
    "\n",
    "def generate_lr_datasets_overwrite():\n",
    "    \"\"\"\n",
    "    Finds every 'ground_truth' folder and recreates 'lr_bicubic' and \n",
    "    'lr_realistic' sibling folders, overwriting any existing LR images.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_frames = 5\n",
    "    scale_factor = 4  # <-- change to 2 for x2 dataset generation\n",
    "    max_subpixel_shift = 0.5\n",
    "\n",
    "    blur_kernel_sizes = [7, 9, 11, 13, 15, 17, 19, 21]\n",
    "    blur_sigma_range = (0.2, 3.0)\n",
    "    noise_gauss_var_range = (0.0001, 0.005)\n",
    "\n",
    "    if not os.path.exists(source_dir):\n",
    "        print(f\"‚ùå ERROR: The source directory does not exist: {source_dir}\")\n",
    "        return\n",
    "\n",
    "    # --- Find all ground truth images ---\n",
    "    print(f\"üîç Scanning for ground truth images in: {source_dir}\")\n",
    "    hr_files_to_process = []\n",
    "    for root, dirs, _ in os.walk(source_dir):\n",
    "        if 'ground_truth' in dirs:\n",
    "            gt_path = os.path.join(root, 'ground_truth')\n",
    "            for f in os.listdir(gt_path):\n",
    "                if f.endswith((\".tif\", \".tiff\")):\n",
    "                    hr_files_to_process.append(os.path.join(gt_path, f))\n",
    "    \n",
    "    total_gt_images = len(hr_files_to_process)\n",
    "    if total_gt_images == 0:\n",
    "        print(\"‚ö†Ô∏è  No 'ground_truth' images found. Please check your 'source_dir' path.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"‚úÖ Found {total_gt_images} ground truth images to process.\")\n",
    "\n",
    "    generated_count = 0\n",
    "    failed_count = 0\n",
    "\n",
    "    for hr_path in tqdm(hr_files_to_process, desc=\"Generating LR Frames\"):\n",
    "        try:\n",
    "            hr_dir = os.path.dirname(hr_path)\n",
    "            hr_filename = os.path.basename(hr_path)\n",
    "            image_set_dir = os.path.dirname(hr_dir)\n",
    "            base_filename = os.path.splitext(hr_filename)[0]\n",
    "\n",
    "            # --- Output directories ---\n",
    "            bicubic_dst_dir = os.path.join(image_set_dir, 'lr_bicubic')\n",
    "            realistic_dst_dir = os.path.join(image_set_dir, 'lr_realistic')\n",
    "            os.makedirs(bicubic_dst_dir, exist_ok=True)\n",
    "            os.makedirs(realistic_dst_dir, exist_ok=True)\n",
    "\n",
    "            # --- Read and normalize HR image ---\n",
    "            hr_image = tifffile.imread(hr_path)\n",
    "            if hr_image.dtype != np.uint8:\n",
    "                hr_image = cv2.normalize(hr_image, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "            rows, cols = hr_image.shape\n",
    "\n",
    "            # --- Generate multiple LR variations ---\n",
    "            for i in range(num_frames):\n",
    "                # Subpixel shift\n",
    "                shift_x = np.random.uniform(-max_subpixel_shift, max_subpixel_shift)\n",
    "                shift_y = np.random.uniform(-max_subpixel_shift, max_subpixel_shift)\n",
    "                M = np.float32([[1, 0, shift_x], [0, 1, shift_y]])\n",
    "                shifted_hr = cv2.warpAffine(hr_image, M, (cols, rows), borderMode=cv2.BORDER_REFLECT_101)\n",
    "\n",
    "                lr_height = rows // scale_factor\n",
    "                lr_width = cols // scale_factor\n",
    "\n",
    "                # --- Bicubic downsample ---\n",
    "                bicubic_downsampled = cv2.resize(shifted_hr, (lr_width, lr_height), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "                # --- Realistic downsample (with blur + noise) ---\n",
    "                kernel_size = random.choice(blur_kernel_sizes)\n",
    "                sigma = random.uniform(*blur_sigma_range)\n",
    "                gauss_var = random.uniform(*noise_gauss_var_range)\n",
    "\n",
    "                blurred_hr = cv2.GaussianBlur(shifted_hr, (kernel_size, kernel_size), sigma)\n",
    "                blurred_hr_float = blurred_hr.astype(np.float32) / 255.0\n",
    "                noisy_gauss = random_noise(blurred_hr_float, mode='gaussian', var=gauss_var, clip=True)\n",
    "                noisy_poisson = random_noise(noisy_gauss, mode='poisson', clip=True)\n",
    "                degraded_hr = (noisy_poisson * 255).astype(np.uint8)\n",
    "                realistic_downsampled = cv2.resize(degraded_hr, (lr_width, lr_height), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "                # --- Save both ---\n",
    "                lr_filename = f\"{base_filename}_{i+1:02d}.png\"\n",
    "                cv2.imwrite(os.path.join(bicubic_dst_dir, lr_filename), bicubic_downsampled)\n",
    "                cv2.imwrite(os.path.join(realistic_dst_dir, lr_filename), realistic_downsampled)\n",
    "\n",
    "            generated_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            failed_count += 1\n",
    "            tqdm.write(f\"\\n‚ö†Ô∏è Could not process {hr_path}. Error: {e}\")\n",
    "\n",
    "    print(\"\\n=========================================\")\n",
    "    print(\"‚úÖ LR Dataset Generation Complete\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(f\"Total Ground Truth Images Found: {total_gt_images}\")\n",
    "    print(f\"   Successfully Generated: {generated_count}\")\n",
    "    print(f\"   Failed (Errors):        {failed_count}\")\n",
    "    print(\"=========================================\")\n",
    "\n",
    "# 3. RUN THE SCRIPT\n",
    "if __name__ == \"__main__\":\n",
    "    generate_lr_datasets_overwrite()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a4dca",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6710216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for all image sets in: C:/Users/milso/Documents/THESIS/Dataset/Training Dataset\n",
      "Performing robust 85:10:5 stratified split...\n",
      "Total image sets found and split: 1092\n",
      "Splitting into: 918 Train, 114 Validation, 60 Test folders.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying to train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 918/918 [00:34<00:00, 26.94it/s]\n",
      "Copying to val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 114/114 [00:04<00:00, 26.70it/s]\n",
      "Copying to test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:02<00:00, 27.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataset copying and splitting complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURE YOUR LOCAL GOOGLE DRIVE PATHS\n",
    "# ==============================================================================\n",
    "# Update this path to where your Google Drive is synced locally.\n",
    "drive_path = \"C:/Users/milso/\" \n",
    "\n",
    "# Path to the dataset you want to split\n",
    "source_dir = os.path.join(drive_path, \"Documents/THESIS/Dataset/Training Dataset\")\n",
    "\n",
    "# Path where the new 'train', 'val', and 'test' folders will be created\n",
    "destination_dir = os.path.join(drive_path, \"Documents/THESIS/Dataset/Split Dataset\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. SCRIPT TO SPLIT THE DATASET (ROBUST STRATIFIED SPLIT)\n",
    "# ==============================================================================\n",
    "def split_dataset():\n",
    "    \"\"\"\n",
    "    Performs a robust 85/10/5 stratified split, treating each image set\n",
    "    (e.g., SIM_gt_a, SIM_gt_b) as an independent unit.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Parameters ---\n",
    "    train_split = 0.85\n",
    "    val_split = 0.10\n",
    "    test_split = 0.05 \n",
    "    random_seed = 42\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    print(f\"Scanning for all image sets in: {source_dir}\")\n",
    "    if not os.path.exists(source_dir):\n",
    "        print(f\"‚ö†Ô∏è ERROR: The source directory was not found: {source_dir}\")\n",
    "        return\n",
    "\n",
    "    # --- Group by specimen, finding ALL folders that contain a 'ground_truth' sub-folder ---\n",
    "    image_sets_by_specimen = defaultdict(list)\n",
    "    for root, dirs, _ in os.walk(source_dir):\n",
    "        # The fundamental unit for splitting is any folder containing 'ground_truth'.\n",
    "        # This correctly treats 'SIM_gt_a' and 'SIM_gt_b' as separate items.\n",
    "        if 'ground_truth' in dirs:\n",
    "            image_set_path = root \n",
    "            relative_path = os.path.relpath(image_set_path, source_dir)\n",
    "            specimen_name = Path(relative_path).parts[0]\n",
    "            image_sets_by_specimen[specimen_name].append(image_set_path)\n",
    "\n",
    "    if not image_sets_by_specimen:\n",
    "        print(\"‚ö†Ô∏è No image sets with a 'ground_truth' folder were found.\")\n",
    "        return\n",
    "\n",
    "    # --- Perform a robust stratified split for each group ---\n",
    "    train_folders, val_folders, test_folders = [], [], []\n",
    "    print(\"Performing robust 85:10:5 stratified split...\")\n",
    "\n",
    "    for specimen, image_set_paths in image_sets_by_specimen.items():\n",
    "        # Shuffling the list of image sets (e.g., [..., 'SIM_gt_a', 'SIM_gt_b', ...])\n",
    "        random.shuffle(image_set_paths)\n",
    "        n_total = len(image_set_paths)\n",
    "\n",
    "        if n_total < 4:\n",
    "            train_folders.extend(image_set_paths)\n",
    "            continue\n",
    "            \n",
    "        n_val = math.ceil(n_total * val_split)\n",
    "        n_test = math.ceil(n_total * test_split)\n",
    "        \n",
    "        if n_val + n_test >= n_total:\n",
    "            n_val = 1\n",
    "            n_test = 1\n",
    "\n",
    "        n_train = n_total - n_val - n_test\n",
    "\n",
    "        train_folders.extend(image_set_paths[:n_train])\n",
    "        val_folders.extend(image_set_paths[n_train : n_train + n_val])\n",
    "        test_folders.extend(image_set_paths[n_train + n_val :])\n",
    "        \n",
    "    total_found = len(train_folders) + len(val_folders) + len(test_folders)\n",
    "    print(f\"Total image sets found and split: {total_found}\")\n",
    "    print(f\"Splitting into: {len(train_folders)} Train, {len(val_folders)} Validation, {len(test_folders)} Test folders.\")\n",
    "\n",
    "    # --- Define and execute the copy process ---\n",
    "    train_dest = os.path.join(destination_dir, 'train')\n",
    "    val_dest = os.path.join(destination_dir, 'val')\n",
    "    test_dest = os.path.join(destination_dir, 'test')\n",
    "\n",
    "    def copy_folders(folder_list, destination_path):\n",
    "        os.makedirs(destination_path, exist_ok=True)\n",
    "        for src_path in tqdm(folder_list, desc=f\"Copying to {os.path.basename(destination_path)}\"):\n",
    "            relative_path = os.path.relpath(src_path, source_dir)\n",
    "            final_dest_path = os.path.join(destination_path, relative_path)\n",
    "            os.makedirs(os.path.dirname(final_dest_path), exist_ok=True)\n",
    "            if os.path.exists(src_path) and not os.path.exists(final_dest_path):\n",
    "                shutil.copytree(src_path, final_dest_path)\n",
    "\n",
    "    copy_folders(train_folders, train_dest)\n",
    "    copy_folders(val_folders, val_dest)\n",
    "    copy_folders(test_folders, test_dest)\n",
    "\n",
    "    print(\"\\n‚úÖ Dataset copying and splitting complete!\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. RUN THE SCRIPT\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    split_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8cf7b",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d019869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abbaaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Base path to start searching ---\n",
    "base_path = r\"C:/Users/milso/Documents/THESIS\\Dataset/Training Dataset\"\n",
    "\n",
    "count = 0  # Counter for removed files\n",
    "\n",
    "# --- Walk through all folders and subfolders ---\n",
    "for root, dirs, files in os.walk(base_path):\n",
    "    for file in files:\n",
    "        if file.lower() == \"desktop.ini\":\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                count += 1\n",
    "                print(f\"üóëÔ∏è Removed: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not remove {file_path}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Done! Removed {count} desktop.ini file(s) in total.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6b4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "# Update this path to your split dataset root\n",
    "# Based on your previous code, it seems to be:\n",
    "dataset_root = \"C:/Users/milso/Documents/THESIS/Dataset/Split Dataset\"\n",
    "\n",
    "# ==============================================================================\n",
    "# COUNTING FUNCTION\n",
    "# ==============================================================================\n",
    "def count_and_display_datasets(root_path, split_name):\n",
    "    target_dir = os.path.join(root_path, split_name)\n",
    "    \n",
    "    if not os.path.exists(target_dir):\n",
    "        print(f\"‚ö†Ô∏è Error: Directory not found: {target_dir}\")\n",
    "        return\n",
    "\n",
    "    bicubic_count = 0\n",
    "    noisy_count = 0 # Maps to 'lr_realistic'\n",
    "\n",
    "    # Walk through the directory tree\n",
    "    for root, dirs, files in os.walk(target_dir):\n",
    "        # Check if the current folder contains the specific subfolders\n",
    "        if 'lr_bicubic' in dirs:\n",
    "            bicubic_count += 1\n",
    "        \n",
    "        # In your file generation code, 'lr_realistic' corresponds to the noisy data\n",
    "        if 'lr_realistic' in dirs:\n",
    "            noisy_count += 1\n",
    "\n",
    "    # --- DISPLAY OUTPUT ---\n",
    "    print(f\"--- {split_name.upper()} FOLDER ---\")\n",
    "    print(f\"{bicubic_count} files in dataset Bicubic\")\n",
    "    print(f\"{noisy_count} files in dataset Noisy\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# ==============================================================================\n",
    "# EXECUTION\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Scanning directory: {dataset_root}\\n\")\n",
    "    \n",
    "    # Count for Train folder\n",
    "    count_and_display_datasets(dataset_root, 'train')\n",
    "    \n",
    "    # Count for Test folder\n",
    "    count_and_display_datasets(dataset_root, 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
